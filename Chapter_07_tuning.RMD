---
title: "Hyperparameter Tuning"
author: "Jeffrey Strickland"
date: "2022-12-25"
output: word_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objectives

The objective of this document is to assess the use of various search methods in finding the optimal values of hyperparameters of a machine learning model. The population-based search methods to be tested are genetic algorithms (GA), differential evolution (DE) and particle swarm optimization (PSO). Grid and random search will also be performed and used as benchmarks.

XGBoost with generalized linear models as learners will be used to predict the compressive strength of concrete based on its composition. The compressive strength of concrete is determined by its age and composition.

The dataset used comes from the research paper Modeling of strength of high performance concrete using artificial neural networks by I-Cheng Yeh published in Cement and Concrete Research, Vol. 28, No. 12, pp.Â 1797-1808 (1998). The dataset can be downloaded through the UCI Machine learning Repository.

The dataset contains 1030 examples and the following features:

    Input Variable: Cement (kg in a m3 mixture)
    Input Variable: Blast Furnace Slag (kg in a m3 mixture)
    Input Variable: Fly Ash (kg in a m3 mixture)
    Input Variable: Water (kg in a m3 mixture)
    Input Variable: Superplasticizer (kg in a m3 mixture)
    Input Variable: Coarse Aggregate (kg in a m3 mixture)
    Input Variable: Fine Aggregate (kg in a m3 mixture)
    Input Variable: Age (days)
    Output Variable: Concrete compressive strength (MPa)

## Install and Load Packages

The pacman package is used to install and load all necessary packages.

```{r}
# install.packages("pacman", verbose = F, quiet = T)
pacman::p_load(caret, tidyverse, readr, readxl, parallel, doParallel, gridExtra, plyr, pso, GA, DEoptim, GGally, xgboost, broom, knitr, kableExtra, tictoc, dplyr, ggplot2, install = F)
```


```{r}
#remove.packages("vctrs")
#install.packages("vctrs")
#library(ggplot2)
#library(vctrs)
```

## Importing the Data

The data was downloaded from the UCI Machine Learning Data Repository:

```{r}
download.file(url = "http://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls", destfile = "Concrete_Data.xls", method = "curl", quiet = TRUE)
```

## Import Data

```{r}
concrete_data <- read_xls(path = "Concrete_Data.xls", sheet = 1)
```

## Rename variables
```{r}
colnames(concrete_data) <- c("Cement", "Slag", "Ash", "Water", "Superplasticizer", "Coarse_Aggregate", "Fine_Aggregate", "Age", "Strength")
```

## Exploratory Data Analyisis

Our first step is to check the structure of the dataset. We do this to determine if there are any transformations that we could make to standardize the data. For instance, if the scales of the individual features are significantly different, our analysis may benefit from "leveling" the scale.

```{r}
glimpse(concrete_data)
```
### Standardize Data

We do see from the glimpse of the data taht scaling is needed. We can do this simply by recalculating the values of the components of the concrete with a range of 0 to 1, using $x_i/\sum_{x = 1}^{n} x_i$.

```{r}
concrete_data[, 1:8] <- t(apply(X = concrete_data[, 1:8], MARGIN = 1, FUN = function(x) {x/sum(x)}))
```

# Print Summary Statistics

Here, we print the summary statistics of the transformed data to ensure that our standardization worked correctly.

```{r}
summary(concrete_data)
```

### Missing Data Check

Next, we calculate the number of NA values in each column, using `colSums()` function with `is.na()` and setting the "remove missing values" (`na,rm`) to `FALSE`.

```{r}
colSums(is.na(concrete_data), na.rm = F)
```

There are no NA values in the dataset. 

### Post Feature Distributions

Next, we examine the histograms of the features to see if any of the distributions are skewed, bi-modal, etc.

```{r}
concrete_data %>% 
  gather(key = Variable, value = Value) %>% 
  ggplot() +
    geom_histogram(aes(x = Value), bins = 20, fill = "blue") +
    facet_wrap(~Variable, scales='free') +
    theme_bw() +
    theme(aspect.ratio = 0.5, axis.title = element_blank(), panel.grid = element_blank())
```

### Skewness Analysis

From the histograms, we can observe possible skewness in Age, Ash, Cement, and Slag. To further analyze skewness, we plot the densities overlayed with the normal density for the respective distributions, using `ggdensity()` from the `ggpubr` package.

```{r}
library(ggpubr)

## Age Distribution
ggdensity(concrete_data, x = 'Age', fill = 'lightgray', title = 'Age', lwd=1, color = 'blue') +
  scale_x_continuous(limits = c(-1,1)) +
  stat_overlay_normal_density(color = 'red', linetype = 'dashed', lwd=1)

## Ash Distribution
ggdensity(concrete_data, x = 'Ash', fill = 'lightgray', title = 'Ash', lwd=1, color = 'blue') +
  scale_x_continuous(limits = c(-1,1)) +
  stat_overlay_normal_density(color = 'red', linetype = 'dashed', lwd=1)

## Cement Distribution
ggdensity(concrete_data, x = 'Cement', fill = 'lightgray', title = 'Cement', lwd=1, color = 'blue') +
  scale_x_continuous(limits = c(-1,1)) +
  stat_overlay_normal_density(color = 'red', linetype = 'dashed', lwd=1)

## Slag Distribution
ggdensity(concrete_data, x = 'Slag', fill = 'lightgray', title = 'Slag', lwd=1, color = 'blue') +
  scale_x_continuous(limits = c(-1,1)) +
  stat_overlay_normal_density(color = 'red', linetype = 'dashed', lwd=1)

## Fine Aggregate Distribution
ggdensity(concrete_data, x = 'Fine_Aggregate', fill = 'lightgray', title = 'Fine Aggregate', lwd=1, color = 'blue') +
  scale_x_continuous(limits = c(-1,1)) +
  stat_overlay_normal_density(color = 'red', linetype = 'dashed', lwd=1)

## Coarse Aggregate Distribution
ggdensity(concrete_data, x = 'Coarse_Aggregate', fill = 'lightgray', title = 'Coarse Aggregate', lwd=1, color = 'blue') +
  scale_x_continuous(limits = c(-1,1)) +
  stat_overlay_normal_density(color = 'red', linetype = 'dashed', lwd=1)

## Water Distribution
ggdensity(concrete_data, x = 'Water', fill = 'lightgray', title = 'Water', lwd=1, color = 'blue') +
  scale_x_continuous(limits = c(-1,1)) +
  stat_overlay_normal_density(color = 'red', linetype = 'dashed', lwd=1)

```

To get numerical score for skewness, we use the `skewness()` function from the `moments` package.

```{r}
library(moments)
print(paste("Age Skewness Score: ",skewness(concrete_data$Age, na.rm = TRUE)))
print(paste("Ash Skewness Score: ",skewness(concrete_data$Ash, na.rm = TRUE)))
print(paste("Cement Skewness Score: ",skewness(concrete_data$Cement, na.rm = TRUE)))
print(paste("Slag Skewness Score: ",skewness(concrete_data$Slag, na.rm = TRUE)))
print(paste("Age Superplasticizer Score: ",skewness(concrete_data$Superplasticizer, na.rm = TRUE)))
print(paste("Age Fine Aggregate Score: ",skewness(concrete_data$Fine_Aggregate, na.rm = TRUE)))
print(paste("Age Coarse Aggregate Score: ",skewness(concrete_data$Coarse_Aggregate, na.rm = TRUE)))
print(paste("Age Water Score: ",skewness(concrete_data$Water, na.rm = TRUE)))
```

So, the one distribution that suffers most from skewness is the Age. However, the usual transformation using a logarithm function will not work due to negative values, and since the scale is well between 0 an 1 (along with the other features), we do not apply a "fix".

### Correlations

Next, we plot a correlation heatmap using the `ggcorr()` function from `GGally` package.

```{r}
ggcorr(concrete_data, label = TRUE, palette = "RdBu", name = "Correlation", hjust = 0.75, layout.exp = 1, label_size = 4, label_round = 2)
```

There seems to be a considerable negative correlation between the amount of water and superplasticizer used. The other features also bear some negative correlation are generally low and do not cause concern. For now, we will try a simple transformation to decrease the correlations, by normalizing the data.


### Visualizing Feature relationships with a scatterplot matrix

We can use a scatterplot matrix (a collection of scatterplots organized in a grid) to understand the relationship between each predictor and the target feature.

```{r, fig.height=3, fig.width=18, warning=FALSE}
ggduo(data = concrete_data, 
      columnsX = 1:8, 
      columnsY = 9, 
      types = list(continuous = "smooth_lm"),
      mapping = ggplot2::aes(color = -Strength, alpha = 0.3)
      ) +
  theme_bw()
```

## Modelling

### Create Training and Test Sets

80% of the samples from the dataset are randomly selected for the training set, the remaining 20% are allocated for testing the models. Stratified partitioning on the target feature will be applied for the creation of these subsets.

### Remove Observations with Missing Values

Although we checked for missing values previously, we have manipulated the data and may have introduce some NaNs.

```{r}
concrete_data <- concrete_data[complete.cases(concrete_data), ]
```

# Average the values of compressive strength of replicate experiments

```{r}
concrete_data <- ddply(.data = concrete_data, 
                       .variables = .(Cement, Slag, Ash, Water, Superplasticizer, `Coarse_Aggregate`, `Fine_Aggregate`, Age), 
                       .fun = function(x) c(Strength = mean(x$Strength)))
```

### Create training and test set using stratified partioning

```{r}
set.seed(1)
training_index <- createDataPartition(y = concrete_data$Strength, p = 0.80)[[1]]
training_set <- concrete_data[training_index, ]
test_set <- concrete_data[-training_index, ]
```

### Check distributtion of strength on training set and test set

```{r}
par(mfrow = c(1, 2))
hist(training_set$Strength, main = "Training Set", xlab = "Concrete Compressive Strength (MPa)", freq = FALSE, col = "skyblue")
hist(test_set$Strength, main = "Test Set", xlab = "Concrete Compressive Strength (MPa)", freq = FALSE, col = "dodgerblue")
```

### Print summary statistics for training and test set

```{r}
bind_rows(summary(training_set$Strength), summary(test_set$Strength)) %>% as_tibble() %>% add_column(Subset = c("Training", "Test"), .before = 1)
```

The distribution of values of the target feature on both the training and test set are similar.

## Training Parameters

### Training Parameters

```{r}
CV_folds <- 5 # number of folds
CV_repeats <- 3 # number of repeats
minimum_resampling <- 5 # minimum number of resamples
```

Parameter tuning and selection will be done using repeated 5-fold cross-validation. Each round of cross-validation will be repeated 3 times. Adaptive resampling will be used for model training with grid and random search.

The caret package will be used for model training, tuning and evaluation.

### Training Settings

```{r}
set.seed(1)

# trainControl object for standard repeated cross-validation
train_control <- caret::trainControl(method = "repeatedcv", number = CV_folds, repeats = CV_repeats, 
                                     verboseIter = FALSE, returnData = FALSE) 

# trainControl object for repeated cross-validation with grid search
adapt_control_grid <- caret::trainControl(method = "adaptive_cv", number = CV_folds, repeats = CV_repeats, 
                                     adaptive = list(min = minimum_resampling, # minimum number of resamples tested before model is excluded
                                                     alpha = 0.05, # confidence level used to exclude parameter settings
                                                     method = "gls", # generalized least squares
                                                     complete = TRUE), 
                                     search = "grid", # execute grid search
                                     verboseIter = FALSE, returnData = FALSE) 

# trainControl object for repeated cross-validation with random search
adapt_control_random <- caret::trainControl(method = "adaptive_cv", number = CV_folds, repeats = CV_repeats, 
                                     adaptive = list(min = minimum_resampling, # minimum number of resamples tested before model is excluded
                                                     alpha = 0.05, # confidence level used to exclude parameter settings
                                                     method = "gls", # generalized least squares
                                                     complete = TRUE), 
                                     search = "random", # execute random search
                                     verboseIter = FALSE, returnData = FALSE) 
```

## Model Training

Extreme Gradient Boosting (XGBoost) will be used to create the regression models. XGBoost is similar to gradient boosting but has the capacity to do parallel computation on a single machine and perform regularization to avoid overfitting. It was developed by Tianqi Chen. Additional advantages of the XGBoost algorithm include its internal cross-validation function, its ability to handle missing values, its flexibility and its ability to prune the tree until the improvement in loss function is below a threshold.

Similar to GBM, XGBoost uses the errors of previous models to reduce them on the next iteration. The final model is a weighted combination of the models obtained on previous iterations. XGBoost has several tuning parameters some of which depend on the type of booster used (CART or generalized linear model) while others are general, regularization and learning task parameters.

Models will be created using generalized linear models as learners.

When using the xgbLinear method in caret, there are four hyperparameters to optimize:

    nrounds: number of boosting iterations
    eta: step size shrinkage
    lambda: L2 Regularization
    alpha: L1 Regularization

The method above requires the xgboost package.

## Grid Search

### Create grid

```{r}
XGBoost_Linear_grid <- expand.grid(
      nrounds = c(50, 100, 250, 500), # number of boosting iterations
      eta = c(0.01, 0.1, 1),  # learning rate, low value means model is more robust to overfitting
      lambda = c(0.1, 0.5, 1), # L2 Regularization (Ridge Regression)
      alpha =  c(0.1, 0.5, 1) # L1 Regularization (Lasso Regression)
) 
```

Train XGBoost models with grid search. 108 different combinations of hyperparameter values are tested. Training will be done using adaptive resampling with a minimum resampling number of 5.

```{r}
GS_T0 <- Sys.time()
cluster <- makeCluster(detectCores() - 1) # number of cores, convention to leave 1 core for OS
registerDoParallel(cluster) # register the parallel processing

set.seed(1); 
# Train model with grid search
GS_XGBoost_Linear_model <- caret::train(Strength ~., 
                           data = training_set,
                           method = "xgbLinear",
                           trControl = adapt_control_grid,
                           verbose = FALSE, 
                           silent = 1,
                           # tuneLength = 20
                           tuneGrid = XGBoost_Linear_grid
                           )

stopCluster(cluster) # shut down the cluster 
registerDoSEQ(); #  force R to return to single threaded processing
GS_T1 <- Sys.time()
GS_T1-GS_T0
```

Time difference of 2.357548 mins

```{r}
GS_XGBoost_Linear_model
```

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nrounds = 500, lambda =
 0.5, alpha = 1 and eta = 0.01.
 
```{r}
GS <- GS_XGBoost_Linear_model
print(paste("MAE =:", max(GS$results$MAE)))
print(paste("RMSE =:", max(GS$results$RMSE)))
print(paste("R-squared =:", max(GS$results$Rsquared)))
```




```{r}
saveRDS(object = GS_XGBoost_Linear_model, file = "C:/Users/jeff/Documents/R/GS_XGBoost_Linear_model.rds")
saveRDS(object = GS_XGBoost_Linear_model$finalModel, file = paste0("C:/Users/jeff/Documents/R/GS_XGBoost_Linear_model.rds_", class(GS_XGBoost_Linear_model$finalModel)[1],".rds"))
```

The best model from the grid search has an average cross-validated RMSE of 4.683. This model has 500 iterations and a alpha and lambda of 1 and 0.5, respectively. The step size shrinkage (eta) is 0.01.

The effect of the hyperparameters values tested on the cross-validation statistics can be seen on the charts below.

```{r, fig.height = 8, fig.width = 6}
# library(tidyverse, quietly = T, verbose = F)

RMSE_plot_XGBoost_Linear <- GS_XGBoost_Linear_model$results %>% 
  dplyr::select(nrounds, lambda, alpha, eta, RMSE) %>% 
  gather(key = Parameter, value = Value, -nrounds, -lambda, -alpha, -eta) %>% 
  filter(Parameter == "RMSE") %>% 
  ggplot(mapping = aes(x = nrounds, y = Value, col = factor(alpha), shape = factor(lambda))) +
    geom_line(size = 1) +
    geom_point(size = 2) +
    facet_wrap(~eta) +
    labs(title = "Average Cross-validated RMSE of XGBoost Linear Model",
       subtitle = "Horizontal tiles representative of step size shrinkage",
       x = "# Boosting Iterations",
       y = "RMSE",
       col = "alpha",
       shape = "lambda"
       ) +
  theme_bw() +
  theme(
        # panel.grid = element_blank(),
        legend.title = element_text(size = 10, face="bold"),
        legend.text = element_text(size = 9),
        plot.title = element_text(size=16),
        axis.title=element_text(size=10, face="bold"),
        axis.text.x = element_text(angle = 0)) +
  scale_x_continuous(breaks = unique(GS_XGBoost_Linear_model$results$nrounds)) +
  scale_color_brewer(type = "qual", palette = "Set1") 

MAE_plot_XGBoost_Linear <- GS_XGBoost_Linear_model$results %>% 
  dplyr::select(nrounds, lambda, alpha, eta, MAE) %>% 
  gather(key = Parameter, value = Value, -nrounds, -lambda, -alpha, -eta) %>% 
  filter(Parameter == "MAE") %>% 
  ggplot(mapping = aes(x = nrounds, y = Value, col = factor(alpha), shape = factor(lambda))) +
    geom_line(size = 1) +
    geom_point(size = 2) +
    facet_wrap(~eta) +
    labs(title = "Average Cross-validated MAE of XGBoost Linear Model",
       subtitle = "Horizontal tiles representative of step size shrinkage",
       x = "# Boosting Iterations",
       y = "MAE",
       col = "alpha",
       shape = "lambda"
       ) +
  theme_bw() +
  theme(
        # panel.grid = element_blank(),
        legend.title = element_text(size = 10, face="bold"),
        legend.text = element_text(size = 9),
        plot.title = element_text(size=16),
        axis.title=element_text(size=10, face="bold"),
        axis.text.x = element_text(angle = 0)) +
  scale_x_continuous(breaks = unique(GS_XGBoost_Linear_model$results$nrounds)) +
  scale_color_brewer(type = "qual", palette = "Set1") 

Rsquared_plot_XGBoost_Linear <- GS_XGBoost_Linear_model$results %>% 
  dplyr::select(nrounds, lambda, alpha, eta, Rsquared) %>% 
  gather(key = Parameter, value = Value, -nrounds, -lambda, -alpha, -eta) %>% 
  filter(Parameter == "Rsquared") %>% 
  ggplot(mapping = aes(x = nrounds, y = Value, col = factor(alpha), shape = factor(lambda))) +
    geom_line(size = 1) +
    geom_point(size = 2) +
    facet_wrap(~eta) +
    labs(title = "Average Cross-validated R-squared of XGBoost Linear Model",
       subtitle = "Horizontal tiles representative of step size shrinkage",
       x = "# Boosting Iterations",
       y = "R-squared",
       col = "alpha",
       shape = "lambda"
       ) +
  theme_bw() +
  theme(
        # panel.grid = element_blank(),
        legend.title = element_text(size = 10, face="bold"),
        legend.text = element_text(size = 9),
        plot.title = element_text(size=16),
        axis.title=element_text(size=10, face="bold"),
        axis.text.x = element_text(angle = 0)) +
  scale_x_continuous(breaks = unique(GS_XGBoost_Linear_model$results$nrounds)) +
  # scale_y_continuous(limits = c(0,1), expand = c(0,0), breaks = seq(0,1,0.1)) +
  scale_color_brewer(type = "qual", palette = "Set1") 

grid.arrange(grobs = list(RMSE_plot_XGBoost_Linear, MAE_plot_XGBoost_Linear, Rsquared_plot_XGBoost_Linear), ncol = 1, nrow = 3)
```

### Grid Search Cross-Validation

#### Cross Validation Custom Functions

Custom functions to plot observed, predicted and residual values for each method.

```{r}
library(ggplot2, quietly = T, verbose = F)

# Function to plot observed vs predicted values
predicted_observed_plot <- function(predicted_val, observed_val, residual_val, model_name = "", R_squared, ...) {
  
  plot <- ggplot(mapping = aes(x = predicted_val, y = observed_val, col = abs(residual_val))) +
  geom_point(alpha = 0.9, size = 2) +
  geom_abline(intercept = 0, slope = 1) +
    # facet_wrap(~) +
    labs(title = paste0(model_name, "\nPredicted vs Observed: Test Set"),
         subtitle = paste0("R-squared: ", R_squared),
         x = "Predicted",
         y = "Observed",
         col = "Absolute Deviation") +
  theme_bw() +
  theme(aspect.ratio = 0.9, panel.grid.minor.x = element_blank(), legend.title = element_text(size = 10, face="bold"), legend.text = element_text(size = 9), plot.title = element_text(size=12, face="bold"), axis.title=element_text(size=10, face="bold"), axis.text.x = element_text(angle = 0), legend.position = "none") +
  # scale_x_continuous(expand = c(0,0)) +
  # scale_y_continuous(expand = c(0,0)) + 
  coord_equal() + scale_color_viridis_c(direction = -1)

  return (plot)
}

# Function to plot residuals
residuals_plot <- function(predicted_val, residual_val, model_name = "", MAE, RMSE, ...) {

  plot <- ggplot(mapping = aes(x = predicted_val, y = residual_val, col = abs(residual_val))) +
  geom_point(alpha = 0.9, size = 2) +
  geom_abline(intercept = 0, slope = 0) +
    # facet_wrap(~) +
    labs(
       title = paste0(model_name, "\nResiduals: Test Set"),
       subtitle = paste0("RMSE: ", RMSE, ", MAE: ", round(MAE, 3)),
       x = "Predicted",
       y = "Residual",
       col = "Absolute Deviation"
       ) +
  theme_bw() +
  theme(aspect.ratio = 0.9, panel.grid.minor.x = element_blank(), legend.title = element_text(size = 10, face="bold"), legend.text = element_text(size = 9), plot.title = element_text(size=12, face="bold"), axis.title=element_text(size=10, face="bold"), axis.text.x = element_text(angle = 0), legend.position = "none") +
  # scale_x_continuous(expand = c(0,0)) +
  # scale_y_continuous(expand = c(0,0)) +
  coord_equal() + scale_color_viridis_c(direction = -1)

  return (plot)
}
```

#### Performance on Test Set

```{r}
# Make predictions on test set
test_set$XGBoost_Linear_GS <- predict(GS_XGBoost_Linear_model, test_set)
# Calculate Residuals on test set
test_set$XGBoost_Linear_GS_residual <- test_set$Strength - test_set$XGBoost_Linear_GS

# Calculate test set R-squared, RMSE, MAE
R_squared <- round(cor(test_set$XGBoost_Linear_GS, test_set$Strength), 4)
RMSE <- signif(RMSE(pred = test_set$XGBoost_Linear_GS, obs = test_set$Strength, na.rm = T), 6)
MAE <- signif(MAE(pred = test_set$XGBoost_Linear_GS, obs = test_set$Strength), 6)

GS_Test_Set_Statistics <- cbind(RMSE, MAE, R_squared)
print(paste("RMSE = ", RMSE))
print(paste("MAE = ", MAE))
print(paste("R_squared = ", R_squared))
```

# Plot predicted vs observed values and residuals

```{r}
XGBoost_Linear_GS_pred_obs <- predicted_observed_plot(predicted_val =
                              test_set$XGBoost_Linear_GS,observed_val = test_set$Strength, 
                              residual_val = test_set$XGBoost_Linear_GS_residual, 
                              R_squared = R_squared, model_name = "Grid Search")
XGBoost_Linear_GS_residuals <- residuals_plot(predicted_val = test_set$XGBoost_Linear_GS, 
                              observed_val = test_set$Strength, 
                              residual_val = test_set$XGBoost_Linear_GS_residual, MAE = MAE, 
                              RMSE = RMSE, model_name = "Grid Search")

XGBoost_Linear_GS_pred_obs
XGBoost_Linear_GS_residuals
```

## Random Search

Train XGBoost models with random search. The same number of different combinations of hyperparameters used in the grid search (108) will be tested. Training will be done using adaptive resampling with a minimum resampling of 5.

### Training Parameters

```{r}
CV_folds <- 5 # number of folds
CV_repeats <- 3 # number of repeats
minimum_resampling <- 5 # minimum number of resamples
```

### Train the Model

```{r}
n_combinations <- nrow(XGBoost_Linear_grid)

RS_T0 <- Sys.time()
cluster <- makeCluster(detectCores() - 1) # number of cores, convention to leave 1 core for OS
registerDoParallel(cluster) # register the parallel processing

set.seed(1); 
RS_XGBoost_Linear_model <- caret::train(Strength ~., 
                          data = training_set,
                          method = "xgbLinear",
                          trControl = adapt_control_random,
                          verbose = FALSE, 
                          silent = 1,
                          tuneLength = n_combinations
                          )

stopCluster(cluster) # shut down the cluster 
registerDoSEQ(); #force R to return to single threaded processing
RS_T1 <- Sys.time()
RS_T1-RS_T0
```

Time difference of 1.2607 mins

```{r}
RS_XGBoost_Linear_model
```

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nrounds = 64, lambda =
 0.9110254, alpha = 0.001859848 and eta = 0.5293561.
 
```{r}
RMSE = min(RS_XGBoost_Linear_model$results$RMSE)
MAE = min(RS_XGBoost_Linear_model$results$MAE)
R_squared = max(RS_XGBoost_Linear_model$results$Rsquared)
print(paste("RMSE = ", RMSE))
print(paste("MAE = ", MAE))
print(paste("R_squared = ", R_squared))
```

```{r}
saveRDS(object = RS_XGBoost_Linear_model, 
        file = "C:/Users/jeff/Documents/R/GS_XGBoost_Linear_model.rds")
saveRDS(object = RS_XGBoost_Linear_model$finalModel, 
        file = paste0("C:/Users/jeff/Documents/R/GS_XGBoost_Linear_model.rds_", 
                      class(RS_XGBoost_Linear_model$finalModel)[1],".rds"))
```

The best model from the grid search has an average cross-validated RMSE of 4.717. This model has 64 iterations and a alpha and lambda of 0.0018598 and 0.9110254, respectively. The step size shrinkage (eta) is 0.5293561.

### Random Search Cross-Validation


```{r}
# Make predictions on test set
test_set$XGBoost_Linear_RS <- predict(RS_XGBoost_Linear_model, test_set)
# Calculate Residuals on test set
test_set$XGBoost_Linear_RS_residual <- test_set$Strength - test_set$XGBoost_Linear_RS

# Calculate test set R-squared, RMSE, MAE
R_squared <- round(cor(test_set$XGBoost_Linear_RS, test_set$Strength), 4)
RMSE <- signif(RMSE(pred = test_set$XGBoost_Linear_RS, obs = test_set$Strength, na.rm = T), 6)
MAE <- signif(MAE(pred = test_set$XGBoost_Linear_RS, obs = test_set$Strength), 6)

RS_Test_Set_Statistics <- cbind(RMSE, MAE, R_squared)
print(paste("RMSE = ", RMSE))
print(paste("MAE = ", MAE))
print(paste("R_squared = ", R_squared))
```

# Plot predicted vs observed values and residuals

```{r}
XGBoost_Linear_RS_pred_obs <- predicted_observed_plot(predicted_val =
                              test_set$XGBoost_Linear_RS,observed_val = test_set$Strength, 
                              residual_val = test_set$XGBoost_Linear_RS_residual, 
                              R_squared = R_squared, model_name = "Random Search")
XGBoost_Linear_RS_residuals <- residuals_plot(predicted_val = test_set$XGBoost_Linear_RS, 
                              observed_val = test_set$Strength, 
                              residual_val = test_set$XGBoost_Linear_RS_residual, MAE = MAE, 
                              RMSE = RMSE, model_name = "Random Search")

XGBoost_Linear_RS_pred_obs
XGBoost_Linear_RS_residuals
```

```{r}
print(paste("Mean Train Strength =", mean(concrete_data$Strength)))
print(paste("Mean Test Strength = ", mean(test_set$XGBoost_Linear_RS)))
```


## Differential Evolution

Differential Evolution (DE) is a population-based search method for multidimensional real-valued functions. Similar to genetic algorithms, DE uses a population of solutions and creates new candidates solutions from parent solutions.

The main difference between genetic algorithms and differential evolution is regarding how new candidate solutions are created. In the latter method, new solutions are created by differential mutation of the population members. Three candidate solutions (e.g. a, b and c) are randomly selected and a mutant parameter vector is created using a simple arithmetic formula, y = a + F(b-c), where F is a positive scaling factor that typically ranges from 0 to 1. This process is repeated for each dimension. The final mutant vector is then used for recombination.
Search for the global minimum of the 2D Ackley function with Differential Evolution.

Search for the global minimum of the 2D Ackley function with Differential Evolution.

The `DEoptim` package will be used for searching optimal concrete mixtures with differential evolution (DE). The package provides the `DEoptim()` function whose main arguments are:

    fn: function to assess the fitness of the solutions
    lower and upper: lower and upper limits for each parameter
    control: list of control parameters for search method (population size, propability of crossover, maximum number of iterations, etc.).

### Search Parameter Settings

First, we set the parameter settings for search DE algorithm.

```{r}
max_iter <- 10 # maximum number of iterations
pop_size <- 10 # population size
```

Prior to run the search method, a custom objective function needs to be created. The average cross-validated root-mean-square error (RMSE) will be used as the parameter to be optimized.

### Solution Assessment Function

Next, we create a custom function for assessing the DE search solutions.

```{r}
eval_function_XGBoost_Linear <- function(x, data, train_settings) {
  
  x1 <- x[1]; x2 <- x[2]; x3 <- x[3]; x4 <- x[4]
  
suppressWarnings(
  XGBoost_Linear_model <- caret::train(Strength ~., 
        data = data,
        method = "xgbLinear",
        trControl = train_settings,
        verbose = FALSE, 
        silent = 1,
        tuneGrid = expand.grid(
              nrounds = round(x1), # number of boosting iterations
              eta = 10^x2, # learning rate, low value means model is more robust to overfitting
              alpha = 10^x3, # L1 Regularization (equivalent to Lasso Regression) on weights
              lambda = 10^x4 # L2 Regularization (equivalent to ridge Regression) on weights
              ) 
         )
)

    return(XGBoost_Linear_model$results$RMSE) # minimize RMSE
}

```

### Define Minimum and Maximum Values

Here, we define the minimum and maximum values for each search parameter input.

```{r}
nrounds_min_max <- c(10,10^3)
eta_min_max <- c(-5,3)
alpha_min_max <- c(-3,1)
lambda_min_max <- c(-3,1)
```

### Train the DE Model

Now, we are ready to train the DE model, using the evaluation function we defined above, along with the parameter upper and lower settings. We define the control function in the code below.

```{r}
set.seed(1)
n_cores <- detectCores()-1

DE_T0 <- Sys.time()
# Run differential evolution algorithm
DE_model_XGBoost_Linear <- DEoptim::DEoptim(
  fn = eval_function_XGBoost_Linear, 
  lower = c(nrounds_min_max[1], eta_min_max[1], alpha_min_max[1], lambda_min_max[1]),
  upper = c(nrounds_min_max[2], eta_min_max[2], alpha_min_max[2], lambda_min_max[2]), 
  control = DEoptim.control(
                            NP = pop_size, # population size
                            itermax = max_iter, # maximum number of iterations
                            CR = 0.5, # probability of crossover
                            storepopfreq = 1, # store every population
                            parallelType = 1 # run parallel processing
                            ),
  data = training_set,
  train_settings = train_control
  )

DE_T1 <- Sys.time()
DE_T1-DE_T0
```

Time difference of 18.51168 mins

### Print Search Results

```{r}
summary(DE_model_XGBoost_Linear)
print(paste("R_Squared =",  max(GS_XGBoost_Linear_model$results$Rsquared)))
```

### Plot results

```{r}
DE_solutions <- DE_model_XGBoost_Linear$optim$bestmem

# Plot results
ggplot(mapping = aes(x = 1:length(DE_model_XGBoost_Linear$member$bestvalit), y = DE_model_XGBoost_Linear$member$bestvalit)) +
    geom_line(col = "grey50", lwd = 1.25) + 
    geom_point(col = "dodgerblue", size = 3) +
    theme_bw() +
    theme(aspect.ratio = 0.9) +
    labs(x = "Iteration", y = "RMSE", title = "Best RMSE value at each iteration", subtitle = "Results using Differential Evolution") +
    scale_x_continuous(breaks = 1:DE_model_XGBoost_Linear$optim$iter, minor_breaks = NULL)
```

The search was completed a found the optimal RMSE value to be 4.323 after 10 iterations and 22 function evaluations.

The final model is trained with the optimal values are found for each hyperparameter.

### Grid of optimal hyperparameter values

```{r}
DE_XGBoost_Linear_grid <- expand.grid(
      nrounds = round(DE_solutions[1]), # learning rate, low value means model is more robust to overfitting
      eta = 10^DE_solutions[2], # number of boosting iterations
      alpha = 10^DE_solutions[3], # L2 Regularization (Ridge Regression)
      lambda = 10^DE_solutions[4] # L1 Regularization (Lasso Regression)
      )
```


```{r}
T0 <- Sys.time()
cluster <- makeCluster(detectCores() - 1) # number of cores, convention to leave 1 core for OS
registerDoParallel(cluster) # register the parallel processing

set.seed(1)
# Train model with optimal values
DE_XGBoost_Linear_model <- caret::train(Strength ~., 
                          data = training_set, 
                          method = "xgbLinear",
                          trControl = train_control,
                          verbose = F, metric = "RMSE", maximize = FALSE,
                          silent = 1,
                          # tuneLength = 1
                          tuneGrid = DE_XGBoost_Linear_grid
                          )

stopCluster(cluster) # shut down the cluster 
registerDoSEQ() #  force R to return to single threaded processing
T1 <- Sys.time()
T1-T0
```

Time difference of 26.43967 secs

```{r}
DE_XGBoost_Linear_model
```


```{r}
saveRDS(object = DE_XGBoost_Linear_model, file = paste0("C:/Users/jeff/Documents/R/GS_XGBoost_Linear_model_PSO_", stringr::str_remove_all(Sys.time(), pattern = ":"),".rds"))
saveRDS(object = DE_XGBoost_Linear_model$finalModel, file = paste0("C:/Users/jeff/Documents/R/GS_XGBoost_Linear_model_PSO_", class(DE_XGBoost_Linear_model$finalModel)[1], "_", stringr::str_remove_all(Sys.time(), pattern = ":"),".rds"))
```

The best model from the search with the differential evolution algorithm has an average cross-validated RMSE of 4.589.

This model has 577 iterations and a alpha and lambda of 0.0015413 and 7.0589095, respectively. The step size shrinkage (eta) is 0.2028573.


### Differential evolution (DE)

#### Make predictions on test set

```{r}
test_set$XGBoost_Linear_DE <- predict(DE_XGBoost_Linear_model, test_set)
# Calculate Residuals on test set
test_set$XGBoost_Linear_DE_residual <- test_set$Strength - test_set$XGBoost_Linear_DE

# Calculate test set R-squared, RMSE, MAE
R_squared <- round(cor(test_set$XGBoost_Linear_DE, test_set$Strength), 4)
RMSE <- signif(RMSE(pred = test_set$XGBoost_Linear_DE, obs = test_set$Strength, na.rm = T), 6)
MAE <- signif(MAE(pred = test_set$XGBoost_Linear_DE, obs = test_set$Strength), 6)

DE_Test_Set_Statistics <- c(RMSE, MAE, R_squared)

print(paste("RMSE = ", DE_Test_Set_Statistics[1]))
print(paste("MAE  = ", DE_Test_Set_Statistics[2]))
print(paste("R_sq = ", DE_Test_Set_Statistics[3]))
```

### Plot Results


```{r}
# Plot predicted vs observed values and residuals
XGBoost_Linear_DE_pred_obs <- predicted_observed_plot(predicted_val = test_set$XGBoost_Linear_DE, observed_val = test_set$Strength, residual_val = test_set$XGBoost_Linear_DE_residual, R_squared = R_squared, model_name = "Differential Evolution Optimization")

XGBoost_Linear_DE_residuals <- residuals_plot(predicted_val = test_set$XGBoost_Linear_DE, observed_val = test_set$Strength, residual_val = test_set$XGBoost_Linear_DE_residual, MAE = MAE, RMSE = RMSE, model_name = "Differential Evolution Optimization")

XGBoost_Linear_DE_pred_obs
XGBoost_Linear_DE_residuals
```

## Particle Swarm Optimization

Particle swarm optimization (PSO) is a population-based search method that belongs to the swarm intelligence family of algorithms. Proposed by Kenned and Eberhart (1995), this method is inspired by the swarm behavior of several animals such as bird flocks, fish schools and bee swarms.

The PSO method searches for the optimal solution by using a population of candidate solutions (also known as particles) that iteratively move across the search space. The movement of a specific particle across the search space is determined by its local best known position but also by the best known position in the search space found by other particles. This results in the whole swarm moving in a self-organized behaviour.

Each particle is defined by its:

    position
    fitness value
    velocity
    previous best position
    previous best position in the neighbourhood

The position of a particle on the next iteration depends on its current position and velocity, while the velocity depends on all of the parameters that define the particle.
A swarm of particles searching for the global minimum. Source: Wikipedia

A swarm of particles searching for the global minimum. Source: Wikipedia

The PSO package will be used to optimize the concrete mixture using particle swarm optimization through the psoptim() function. Prior to run the search method, a custom objective function is created.

### Set parameter settings for search algorithm

```{r PSO-01}
max_iter <- 10 # maximum number of iterations
pop_size <- 10 # population size
```


```{r}
# Create custom function for assessing solutions
eval_function_XGBoost_Linear <- function(x, data, train_settings) {
  
  x1 <- x[1]; x2 <- x[2]; x3 <- x[3]; x4 <- x[4]
  
suppressWarnings(
  # Create dataframe with proportion of each solid component
  XGBoost_Linear_model <- caret::train(Strength ~., 
                                data = data,
                                method = "xgbLinear",
                                trControl = train_settings,
                                #verbose = F, 
                                #silent = 0,
                                tuneGrid = expand.grid(
                                                            nrounds = round(x1), # number of boosting iterations
                                                            eta = 10^x2, # learning rate, low value means model is more robust to overfitting
                                                            alpha = 10^x3, # L1 Regularization (equivalent to Lasso Regression) on weights
                                                            lambda = 10^x4 # L2 Regularization (equivalent to ridge Regression) on weights
                                                            ) 
                                      )
)

    return(XGBoost_Linear_model$results$RMSE) # minimize RMSE

}

# Define minimum and maximum values for each input
nrounds_min_max <- c(5,8)
eta_min_max <- c(-2,2)
alpha_min_max <- c(-2,1)
lambda_min_max <- c(-2,1)
```


```{r}
set.seed(1)
n_cores <- detectCores()-1

PSO_T0 <- Sys.time()
# Run search algorithm
PSO_model_XGBoost_Linear <- pso::psoptim(
  par = rep(NA, 4),
  fn = eval_function_XGBoost_Linear, 
  lower = c(740, 1, 3, lambda_min_max[1]),
  upper = c(740, 1, 3, lambda_min_max[2]), 
  control = list(
                trace = 1, #  produce tracing information on the progress of the optimization
                maxit = 10, # maximum number of iterations
                REPORT = 1, #  frequency for reports
                trace.stats = T,
                s = 10, # Swarm Size,
                maxit.stagnate = round(0.75*max_iter), # maximum number of iterations without improvement
                vectorize = T,
                type = "SPSO2011" # method used
                ),
  data = training_set,
  train_settings = train_control
  )
PSO_T1 <- Sys.time()
PSO_T1-PSO_T0
```


```{r}
PSO_summary <- data.frame(
            Iteration = PSO_model_XGBoost_Linear$stats$it,
            Mean = PSO_model_XGBoost_Linear$stats$f %>% 
                 sapply(FUN = mean),
            Median = PSO_model_XGBoost_Linear$stats$f %>%
                 sapply(FUN = median),
            Best = PSO_model_XGBoost_Linear$stats$error %>% 
                 sapply(FUN = min)
                          )
PSO_summary %>% 
  gather(key = "Parameter", value = "Value", -Iteration) %>% 
  ggplot(mapping = aes(x = Iteration, y = Value, 
                       col = Parameter)) +
    geom_line(lwd = 1) +
    geom_point(size = 3) +
    theme_bw() +
    theme(aspect.ratio = 0.9) +
    scale_x_continuous(breaks = 
                       PSO_model_XGBoost_Linear$stats$it, 
                       minor_breaks = NULL) +
    labs(x = "Iteration", y = "RMSE", title = "RMSE values at each iteration", subtitle = "Results using Particle Swarm Optimization") +
    scale_color_brewer(type = "qual", palette = "Set1")
```


```{r}
# Grid of optimal hyperparameter values
PSO_XGBoost_Linear_grid <- expand.grid(
                                  nrounds = round(PSO_model_XGBoost_Linear$par[1]),  # number of boosting iterations
                                  eta = PSO_model_XGBoost_Linear$par[2], # learning rate, low value means model is more robust to overfitting
                                  alpha = PSO_model_XGBoost_Linear$par[3], # L2 Regularization (Ridge Regression)
                                  lambda = PSO_model_XGBoost_Linear$par[4] # L1 Regularization (Lasso Regression)
                                  )

T0 <- Sys.time()
cluster <- makeCluster(detectCores() - 1) # number of cores, convention to leave 1 core for OS
registerDoParallel(cluster) # register the parallel processing

set.seed(1)
# Train model with optimal values
PSO_XGBoost_Linear_model <- caret::train(Strength ~., 
                          data = training_set, 
                          method = "xgbLinear",
                          trControl = train_control,
                          verbose = F, metric = "RMSE", maximize = FALSE,
                          silent = 1,
                          tuneGrid = PSO_XGBoost_Linear_grid
                          )

stopCluster(cluster) # shut down the cluster 
registerDoSEQ() #  force R to return to single threaded processing
T1 <- Sys.time()
T1-T0
```


```{r}
PSO_XGBoost_Linear_model
```


```{r}
PSO_XGBoost_Linear_model$results %>% arrange(RMSE) %>% .[1,] %>% select(RMSE, MAE, Rsquared, nrounds, eta, lambda, alpha) %>% round(5)
```

### Particle Swarm Optimization (PSO)

```{r}
# Make predictions on test set
test_set$XGBoost_Linear_PSO <- predict(PSO_XGBoost_Linear_model, test_set)
# Calculate Residuals on test set
test_set$XGBoost_Linear_PSO_residual <- test_set$Strength - test_set$XGBoost_Linear_PSO

# Calculate test set R-squared, RMSE, MAE
R_squared <- round(cor(test_set$XGBoost_Linear_PSO, test_set$Strength), 4)
RMSE <- signif(RMSE(pred = test_set$XGBoost_Linear_PSO, obs = test_set$Strength, na.rm = T), 6)
MAE <- signif(MAE(pred = test_set$XGBoost_Linear_PSO, obs = test_set$Strength), 6)

PSO_Test_Set_Statistics <- c(RMSE, MAE, R_squared)
```


```{r}
print(paste("RMSE = ", PSO_Test_Set_Statistics[1]))
print(paste("MAE  = ", PSO_Test_Set_Statistics[2]))
print(paste("R_sq = ", PSO_Test_Set_Statistics[3]))
```


```{r}
# Plot predicted vs observed values and residuals
XGBoost_Linear_PSO_pred_obs <- predicted_observed_plot(predicted_val = test_set$XGBoost_Linear_PSO, observed_val = test_set$Strength, residual_val = test_set$XGBoost_Linear_PSO_residual, R_squared = R_squared, model_name = "Particle Swarm Optimization")

XGBoost_Linear_PSO_residuals <- residuals_plot(predicted_val = test_set$XGBoost_Linear_PSO, observed_val = test_set$Strength, residual_val = test_set$XGBoost_Linear_PSO_residual, MAE = MAE, RMSE = RMSE, model_name = "Particle Swarm Optimization")

XGBoost_Linear_PSO_pred_obs
XGBoost_Linear_PSO_residuals
```


```{r}
saveRDS(object = PSO_XGBoost_Linear_model, file = paste0("C:/Users/jeff/Documents/R/PSO_XGBoost_Linear_model_", stringr::str_remove_all(Sys.time(), pattern = ":"),".rds"))
saveRDS(object = PSO_XGBoost_Linear_model$finalModel, file = paste0("C:/Users/jeff/Documents/R/PSO_XGBoost_Linear_model_", class(PSO_XGBoost_Linear_model$finalModel)[1], "_", stringr::str_remove_all(Sys.time(), pattern = ":"),".rds"))
```

## Model Performance

### Performance on Training Set

The training set statistics and hyperparameters values for each search method tested are summarised on the table below.

### Create summary table

```{r}
Summary_Table_Training <- bind_rows(
  GS_XGBoost_Linear_model$results %>% arrange(RMSE) %>% .[1,] %>% select(RMSE, MAE, Rsquared, nrounds, eta, lambda, alpha) %>% round(5),
  RS_XGBoost_Linear_model$results %>% arrange(RMSE) %>% .[1,] %>% select(RMSE, MAE, Rsquared, nrounds, eta, lambda, alpha) %>% round(5),
  DE_XGBoost_Linear_model$results %>% arrange(RMSE) %>% .[1,] %>% select(RMSE, MAE, Rsquared, nrounds, eta, lambda, alpha) %>% round(5),
  PSO_XGBoost_Linear_model$results %>% arrange(RMSE) %>% .[1,] %>% select(RMSE, MAE, Rsquared, nrounds, eta, lambda, alpha) %>% round(5))

Summary_Table_Training <- Summary_Table_Training %>% 
  add_column(Method = c("Grid Search", "Random Search", "Differential Evolution", "Particle Swarm Optimization"), .before = 1) %>% 
  add_column(`Processing Time` = round(c(GS_T1-GS_T0, RS_T1-RS_T0, DE_T1-GS_T0, PSO_T1-PSO_T0),0))

# Print table
Summary_Table_Training %>% 
  kable(align = "c", caption = "Training Set Statistics and Hyperparameter Values of XGBoost Models.") %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::footnote(general = paste0("Note:\nSummary statistics obtained using ", CV_folds, "-fold cross validation repeated ", CV_repeats, " times.","\nGrid and Random Search  performed with adaptive resampling."), general_title = "\n ")
```

Training Set Statistics and Hyperparameter Values of XGBoost Models. Method 	RMSE 	MAE 	Rsquared 	nrounds 	eta 	lambda 	alpha 	Processing Time
Grid Search 	4.68290 	3.15011 	0.91690 	500 	0.01000 	0.50000 	1.00000 	5 mins
Random Search 	4.71698 	3.21069 	0.91588 	64 	0.52936 	0.91103 	0.00186 	2 mins
Genetic Algorithm 	4.83261 	3.24182 	0.91153 	420 	Inf 	0.00166 	0.00015 	11 mins
Differential Evolution 	4.58926 	3.08924 	0.92027 	577 	0.20286 	7.05891 	0.00154 	37 mins
Particle Swarm Optimization 	4.69397 	3.11996 	0.91683 	740 	3.00000 	1.00000 	1.00000 	24 mins

Note:
Summary statistics obtained using 5-fold cross validation repeated 3 times.
Grid and Random Search performed with adaptive resampling.

Although there are some considerable differences between the values of hyperparameters obtained from each search method, the performance metrics are relatively similar.

Differential Evolution obtained the lowest RMSE value (4.5893).

## Performance on Test Set

### Training Summary Table 

```{r}
Summary_Table_Test <- rbind(
  GS_XGBoost_Linear_model$results %>% arrange(Rsquared) %>% .[1,] %>% select(RMSE, MAE, Rsquared) %>% round(5),
  RS_XGBoost_Linear_model$results %>% arrange(Rsquared) %>% .[1,] %>% select(RMSE, MAE, Rsquared) %>% round(5),
  DE_XGBoost_Linear_model$results %>% arrange(Rsquared) %>% .[1,] %>% select(RMSE, MAE, Rsquared) %>% round(5),
  PSO_XGBoost_Linear_model$results %>% arrange(Rsquared) %>% .[1,] %>% select(RMSE, MAE, Rsquared) %>% round(5), deparse.level = 0 
  ) %>% data.frame()

colnames(Summary_Table_Test) <- c("RMSE", "MAE", "R-squared")
Summary_Table_Test <- Summary_Table_Test %>% add_column(Method = c("Grid Search", "Random Search", "Differential Evolution", "Particle Swarm Optimization"), .before = 1)

# Print table
Summary_Table_Test %>% 
  kable(align = "c", caption = "Test Set Statistics of XGBoost Models.") %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::footnote(general = paste0(""), general_title = "\n ")
```

### Cross V-Validation Summary Table

```{r}
Summary_Table_Test <- rbind(
  GS_Test_Set_Statistics,
  RS_Test_Set_Statistics,
  DE_Test_Set_Statistics,
  PSO_Test_Set_Statistics, deparse.level = 0 
  ) %>% data.frame()

colnames(Summary_Table_Test) <- c("RMSE", "MAE", "R-squared")
Summary_Table_Test <- Summary_Table_Test %>% add_column(Method = c("Grid Search", "Random Search", "Differential Evolution", "Particle Swarm Optimization"), .before = 1)

# Print table
Summary_Table_Test %>% 
  kable(align = "c", caption = "Test Set Statistics of XGBoost Models.") %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::footnote(general = paste0(""), general_title = "\n ")
```

Based on test set statistics, the Differential Evolution obtained the lowest RMSE value (4.936).

```{r}
colnames(Summary_Table_Test) <- c("nrounds", "eta", "lambda",	"alpha")
Summary_Table_Test <- Summary_Table_Test %>% add_column(Method = c("Grid Search", "Random Search", "Differential Evolution", "Particle Swarm Optimization"), .before = 1)

# Print table
Summary_Table_Test %>% 
  kable(align = "c", caption = "Test Set Statistics of XGBoost Models.") %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = T, position = "center") %>%
  kableExtra::footnote(general = paste0(""), general_title = "\n ")
```


```{r}
g1 <- gridExtra::grid.arrange(XGBoost_Linear_GS_pred_obs, XGBoost_Linear_GS_residuals, 
                             XGBoost_Linear_RS_pred_obs, XGBoost_Linear_RS_residuals,
                             ncol = 2)
```




```{r}
g2 <- gridExtra::grid.arrange(XGBoost_Linear_DE_pred_obs, XGBoost_Linear_DE_residuals, 
                             XGBoost_Linear_PSO_pred_obs, XGBoost_Linear_PSO_residuals, 
                             ncol = 2)
```
## Summary

The use of population-based search methods such as genetic algorithms, differential evolution and particle swarm optimation show some potential for finding the optimal values of hyperparameters of regression models.

The use of Differential Evolution resulted on the lowest test set RMSE value of all the methods tested.

Due to the high computational demands, the use of these methods may be more suitable for models with several hyperparameters used on relatively small datasets and/or models that are easily trained. Performing a grid search across a wide range of values for beforehand may also help to narrow the limits of the constraints and make the overall process more efficient.






