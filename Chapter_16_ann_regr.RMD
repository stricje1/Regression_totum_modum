---
title: "ANN Regression"
author: "Jeffrey Strickland"
date: "2022-11-05"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, dpi = 300)
```

# Regression Artificial Neural Network

Regression ANNs predict an output variable as a function of the inputs. The input features (independent variables) can be categorical or numeric types, however, for regression ANNs, we require a numeric dependent variable. If the output variable is a categorical variable (or binary) the ANN will function as a classifier.

Here we introduce a neural network used for numeric predictions.

## Data Set Information:
  
  Prediction of residuary resistance of Space Crafts at the initial design stage is of a great value for evaluating the shipâ€™s performance and for estimating the required propulsive power. Essential inputs include the basic hull dimensions and the boat velocity.

The NASA data set comprises 308 full-scale experiments, which were performed at the NASA Astdromechanics Laboratory for that purpose.
These experiments include 22 different hull forms, derived from a parent form closely related to the Ares I designed by Frans Maas.

## Attribute Information:
  
Variations concern space-craft hull geometry coefficients and the Froude number. The Froude number is a ratio of inertial and gravitational forces. The longitudinal prismatic coefficient, CP, or simply prismatic coefficient is the ratio of the volume of displacement to the volume of a prism having a length equal to the length between perpendiculars and a cross-sectional area equal to the midship sectional area.
  
1. Longitudinal position of the center of buoyancy, adimensional.
2. Prismatic coefficient, adimensional.
3. Length-displacement ratio, adimensional.
4. Beam-draught ratio, adimensional.
5. Length-beam ratio, adimensional.
6. Froude number, adimensional.

The measured variable is the residuary resistance per unit weight of displacement:
  
7. Residuary resistance per unit weight of displacement, adimensional. 

The Froude number is a ratio of inertial and gravitational forces. 

The longitudinal prismatic coefficient, CP, or simply prismatic coefficient is the ratio of the volume of displacement to the volume of a prism having a length equal to the length between perpendiculars and a cross-sectional area equal to the midship sectional area.

The displacement–length ratio (DLR or D/L ratio) is a calculation used to express how heavy a boat is relative to its waterline length.

## Replication Requirements

We require the following packages for the analysis.

```{r}
library(dplyr)
library(tidyverse)
library(neuralnet)
library(GGally)
```

## Data Preparation

Our regression ANN will use the Craft Hydrodynamics data set from UCI’s Machine Learning Repository. The Craft data was provided by Dr. Roberto Lopez email. This data set contains data contains results from 308 full-scale experiments performed at the Delft Ship Hydromechanics Laboratory where they test 22 different hull forms. Their experiment tested the effect of variations in the hull geometry and the ship’s Froude number on the craft’s residuary resistance per unit weight of displacement.

To begin we download the data from UCI.


```{r}
url <- 'https://raw.githubusercontent.com/stricje1/Data/master/craft_astro_data.csv'

Craft_Data <- read_csv(file = url) 

```

Prior to any data analysis lets take a look at the data set.

```{r}
p_ <- GGally::print_if_interactive
pm <- ggpairs(Craft_Data, title = "Scatterplot Matrix of the Features of the Craft Data Set",  ggplot2::aes(colour= ''))
p_(pm)
```

Here we see an excellent summary of the variation of each feature in our data set. Draw your attention to the bottom-most strip of scatter-plots. This shows the residuary resistance as a function of the other data set features (independent experimental values). The greatest variation appears with the Froude Number feature. It will be interesting to see how this pattern appears in the subsequent regression ANNs.

We can also transform the correlation matrix into a correlation plot. A correlation plot (also referred as a correlogram or corrgram in Friendly (2002)) allows to highlight the variables that are most (positively and negatively) correlated. Below an example with the same dataset presented above:

```{r}
# load package
library(ggstatsplot)

# correlogram
ggstatsplot::ggcorrmat(
  data = Craft_Data,
  type = "parametric", # parametric for Pearson, nonparametric for Spearman's correlation
  colors = c("darkred", "white", "steelblue") # change default colors
)
```

Heatmaps are another method of representing data graphically where values are depicted by color, making it easy to visualize complex data and understand it at a glance. There are two prerequisite actions we need to perform before generating a heat map. The first one is to make all data numerical. We did this in an earlier step, since all machine learning models require the data to be encoded into numerical format. The second is to change the structure of the data from data frame to matrix. Once this is accomplished, we create the heatmap.

```{r}
Craft_Data  <- as.matrix(Craft_Data)
corr <- cor(Craft_Data)
par(oma=c(6,0,0,2))
par(mar=c(2,0,0,2))
heatmap(corr, Colv = NA, Rowv = NA, scale="column")
```

Prior to regression ANN construction we first must split the Craft data set into test and training data sets. Before we split, first scale each feature to fall in the [0,1]
interval.

```{r}
# Scale the Data
scale01 <- function(x){
  (x - min(x)) / (max(x) - min(x))
}

Craft_Data <- as.data.frame(Craft_Data) %>%
  mutate_all(scale01)
write_csv(Craft_Data,"Craft_Data.csv")

# Split into test and train sets
set.seed(12345)
Craft_Data_Train <- sample_frac(Craft_Data, replace = FALSE, size = 0.65)
Craft_Data_Test <- anti_join(Craft_Data, Craft_Data_Train)
```

The scale01() function maps each data observation onto the [0,1]

interval as called in the dplyr mutate_all() function. We then provided a seed for reproducible results and randomly extracted (without replacement) 80% of the observations to build the Craft_Data_Train data set. Using dplyr’s anti_join() function we extracted all the observations not within the Craft_Data_Train data set as our test data set in Craft_Data_Test.

## Regression ANN Model 1

To begin we construct a 1-hidden layer ANN with 1 neuron, the simplest of all neural networks.

```{r}
set.seed(12321)
Craft_NN1 <- neuralnet(Residuary_Resist ~ 
                         LongPos_COB + 
                         Prismatic_Coeff + 
                         Len_Disp_Ratio + 
                         Beam_Draut_Ratio + 
                         Length_Beam_Ratio +
                         Froude_Num, data = 
                         Craft_Data_Train)
```

The Craft_NN1 is a list containing all parameters of the regression ANN as well as the results of the neural network on the test data set. To view a diagram of the Craft_NN1 use the plot() function.

```{r}
plot(Craft_NN1, rep = 'best')
```

This plot shows the weights learned by the Craft_NN1 neural network, and displays the number of iterations before convergence, as well as the SSE of the training data set. To manually compute the SSE you can use the following:

```{r}
NN1_Train_SSE <- sum((Craft_NN1$net.result - Craft_Data_Train[ 7])^2)/2
paste("SSE: ", round(NN1_Train_SSE, 4))
```

This SSE is the error associated with the training data set. A superior metric for estimating the generalization capability of the ANN would be the SSE of the test data set. Recall, the test data set contains observations not used to train the Craft_NN1 ANN. To calculate the test error, we first must run our test observations through the Craft_NN1 ANN. This is accomplished with the neuralnet package compute() function, which takes as its first input the desired neural network object created by the neuralnet() function, and the second argument the test data set feature (independent variable(s)) values.

```{r}
Test_NN1_Output <- compute(Craft_NN1, Craft_Data_Test[, 1:6])$net.result
NN1_Test_SSE <- sum((Test_NN1_Output - Craft_Data_Test[, 7])^2)/2
NN1_Test_SSE
```

This SSE is the error associated with the training data set. A superior metric for estimating the generalization capability of the ANN would be the SSE of the test data set. Recall, the test data set contains observations not used to train the Craft_NN1 ANN. To calculate the test error, we first must run our test observations through the Craft_NN1 ANN. This is accomplished with the neuralnet package compute() function, which takes as its first input the desired neural network object created by the neuralnet() function, and the second argument the test data set feature (independent variable(s)) values.

```{r}
set.seed(12321)
Craft_NN1 <- neuralnet(Residuary_Resist ~ 
                         LongPos_COB + 
                         Prismatic_Coeff + 
                         Len_Disp_Ratio + 
                         Beam_Draut_Ratio + 
                         Length_Beam_Ratio +
                         Froude_Num, data = 
                         Craft_Data_Train)

NN1_Train_SSE <- sum((Craft_NN1$net.result - Craft_Data_Train[7])^2)/2
paste("SSE: ", round(NN1_Train_SSE, 4))

Test_NN1_Output <- compute(Craft_NN1, Craft_Data_Test[, 1:6])$net.result
NN1_Test_SSE <- sum((Test_NN1_Output - Craft_Data_Test[7])^2)/2
NN1_Test_SSE
```

The compute() function outputs the response variable, in our case the Residuary_Resist, as estimated by the neural network. Once we have the ANN estimated response we can compute the test SSE. Comparing the test error of 0.0084 to the training error of 0.0361 we see that in our case our test error is smaller than our training error.

## Regression Hyperparameters

We have constructed the most basic of regression ANNs without modifying any of the default hyperparameters associated with the neuralnet() function. We should try and improve the network by modifying its basic structure and hyperparameter modification.

## Regression ANN Model 2

To begin we will add depth to the hidden layer of the network, then we will change the activation function to the logistic to determine if these modifications can improve the cross-validation data set SSE and decrease the difference in SSE between the train and cross-validation set. When using an activation function, we can manually rescale the data using the rescale package, or we can allow the neural network to transform the data as is needed for it to train a model and reduce the SSE. For the purposes of this example, we will use the same random seed for reproducible results, generally this is not a best practice.


```{r}
# 2-Hidden Layers, Layer-1 4-neurons, Layer-2, 1-neuron, 
# logistic activation function
set.seed(12321)

Craft_NN2 <- neuralnet(Residuary_Resist ~ 
                         LongPos_COB + 
                         Prismatic_Coeff + 
                         Len_Disp_Ratio + 
                         Beam_Draut_Ratio + 
                         Length_Beam_Ratio + 
                         Froude_Num, 
                       data = Craft_Data_Train, 
                       hidden = c(4, 1), 
                       act.fct = "logistic")

## Training Error
NN2_Train_SSE <- sum((Craft_NN2$net.result - 
                        Craft_Data_Train[7])^2)/2

## Test Error
Test_NN2_Output <- compute(Craft_NN2, 
                           Craft_Data_Test[, 1:6])$net.result
NN2_Test_SSE <- sum((Test_NN2_Output - 
                       Craft_Data_Test[7])^2)/2

paste("NN2 Train SSE: ", round(NN2_Train_SSE, 4))
paste("NN2 Test SSE: ", round(NN2_Test_SSE, 4))

```

## Regression ANN Model 3

To continue with a third model, we will add depth to the hidden layer of the network, then we will change the activation function to the hyperbolic tangent (tanh) to determine if these modifications can improve the cross-validation data set SSE and decrease the difference in SSE between the train and cross-validation set. When using an activation function, we can manually rescale the data using the rescale package, or we can allow the neural network to transform the data as is needed for it to train a model and reduce the SSE. For the purposes of this example, we will use the same random seed for reproducible results, generally this is not a best practice.


```{r}
# 3-Hidden Layers, Layer-1 2-neurons, Layer-2, 2-neuron, Layer-3, 1-neuron
# tanh activation function

set.seed(12321)
Craft_NN3 <- neuralnet(Residuary_Resist ~ 
                         LongPos_COB + 
                         Prismatic_Coeff + 
                         Len_Disp_Ratio + 
                         Beam_Draut_Ratio + 
                         Length_Beam_Ratio + 
                         Froude_Num, 
                       data = Craft_Data_Train, 
                       hidden = c(2, 2, 1), 
                       act.fct = "tanh")

## Training Error
NN3_Train_SSE <- sum((Craft_NN3$net.result - 
                        Craft_Data_Train[7])^2)/2

## Test Error
Test_NN3_Output <- compute(Craft_NN3, 
                           Craft_Data_Test[, 1:6])$net.result
NN3_Test_SSE <- sum((Test_NN3_Output - 
                       Craft_Data_Test[7])^2)/2

NN3_Train_SSE 
NN3_Test_SSE
```


# Bar Plot Model Comparison

Now, we construct a bar chart to compare the three models using the Train SSE, CV SSE, and the Difference for each model.

```{r}
Diff_NN1 <- NN1_Train_SSE - NN1_Test_SSE
Diff_NN2 <- NN2_Train_SSE - NN2_Test_SSE
Diff_NN3 <- NN3_Train_SSE - NN3_Test_SSE

Regression_NN_Errors <- tibble(Network = 
                rep(c("NN1", "NN2", "NN3"), each = 3), 
                DataSet = rep(c("Diff", "Train", "Test"), time = 3), 
                SSE = c(Diff_NN1, NN1_Train_SSE, NN1_Test_SSE, 
                        Diff_NN2, NN2_Train_SSE, NN2_Test_SSE, 
                        Diff_NN3, NN3_Train_SSE, NN3_Test_SSE 
                                ))

Regression_NN_Errors %>% 
  ggplot(aes(Network, SSE, fill = DataSet)) + 
  geom_col(position = "dodge") + 
  ggtitle("Regression ANN's SSE")
```

As evident from the plot, we see that the best regression ANN we found was Craft_NN2 with a training and test SSE of 0.0188 and 0.0057. We make this determination by the value of the training and test SSEs only. Craft_NN2’s structure is presented here:

```{r} 
plot(Craft_NN2, rep = "best")
```

We have looked at one ANN for each of the hyperparameter settings. Generally, researchers look at more than one ANN for a given setting of hyperparameters. This capability is built into the neuralnet package using the rep argument in the neuralnet() function. Using the Craft_NN2 hyperparameters we construct 10 different ANNs, and select the best of the 10.

```{r}
set.seed(12321)
Craft_NN2 <- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio + Froude_Num, 
                       data = Craft_Data_Train, 
                       hidden = c(4, 1), 
                       act.fct = "logistic", 
                       rep = 10)

plot(Craft_NN2, rep = "best", lwd = 2)
```

By setting the same seed, prior to running the 10 repetitions of ANNs, we force the software to reproduce the exact same Craft_NN2 ANN for the first replication. The subsequent 9 generated ANNs, use a different random set of starting weights. Comparing the ‘best’ of the 10 repetitions, to the Craft_NN2, we observe a decrease in training set error indicating we have a superior set of weights.

## Wrapping Up

We have briefly covered regression ANNs in this tutorial. In the next tutorial we will cover classification ANNs. The neuralnet package used in this tutorial is one of many tools available for ANN implementation in R. Others include:
  
nnet
autoencoder
caret
RSNNS
h2o

Before you move on to the next tutorial, test your new knowledge on the exercises that follow.

Why do we split the Craft data into a training and test data sets?
  Re-load the Craft Data from the UCI Machine learning repository Craft data without scaling. Run any regression ANN. What happens? Why do you think this happens?
  After completing exercise question 1, re-scale the Craft data. Perform a simple linear regression fitting Residuary_Resist as a function of all other features. Now run a regression neural network (see 1st Regression ANN section). Plot the regression ANN and compare the weights on the features in the ANN to the p-values for the regressors.
Build your own regression ANN using the scaled Craft data modifying one hyperparameter. Use ?neuralnet to see the function options. Plot your ANN.






